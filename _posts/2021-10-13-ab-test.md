---
title: A/B 테스트 올바르게 하기
tags: [ab-test, recsys]
category: 통계
aside:
  toc: true
show_category: true
---

A/B 테스트는 훌륭한 검정 방법이지만 올바르게 쓰지 않으면 아무런 의미가 없습니다.

<!--more-->

## 🚥 Before a Test — Not Every Idea Is Worth Testing

- A/B 테스트는 훌륭한 도구지만 **모든 아이디어를 테스트할 순 없음**
    - 테스트 비용이 높거나, Early phase에 있는 회사들은 리소스에 제약이 있기 때문
- **샘플 인터뷰 질문**
  
    >   *E-커머스 웹페이지가 있고, 전환율을 증가시키기 위한 몇 개의 아이디어가 있다고 가정하자.
    >   복수의 아이템을 체크아웃 가능하게 하거나, 비회원에게도 체크아웃을 허용하거나, "구매" 버튼의 크기와 색을 변경하는 등의 아이디어가 있다.
    >   당신이라면 어떤 아이디어에 투자할 지 어떻게 정할 것인가?*
    
    - 한 가지 방법으로는 과거 데이터에 대해 **양적 분석(Quantitative Analysis)**을 수행하여 각 아이디어에 대한 **기회 평가(Opportunity Sizing)**를 하는 것
    - 만약 매우 적은 수의 사용자만이 한 개 이상의 아이템을 구매한다면 복수의 아이템을 체크아웃하는 기능은 개발 비용 대비 가치가 없음
        - 오히려 사용자들의 구매 행동을 조사하여 왜 복수의 아이템을 구매하지 않는지 알아보는 것이 더 중요함
    - 이러한 분석들이 A/B 테스트를 할만한 좋은 후보들을 고르는 방향을 제시함
    - 다각도 평가를 위해선 **질적 분석(Qualitative Analysis)**를 수행할 수도 있음
        - 포커스 그룹을 선정하여 피드백을 얻고, 이런 피드백이 사용자의 페인 포인트나 선호도에 대한 인사이트를 제공해 줌

## 🖌 Designing A/B Tests

### ⏰ How long to run a test?

- 테스트 수행 기간을 정하기 위해서는 테스트를 위한 샘플 사이즈를 알아야 하며, 요구되는 파라미터는 다음과 같음
    - Type II 에러율 $\beta$ 또는 Power ($1-\beta$)
    - 유의 수준 $\alpha$
    - 최소 검출 가능 효과 (Minimum detectable effect, MDE)
- The rule of thumb for sample size is the following, where $\sigma$ is the sample variance and $\delta$ is the difference betwen treatment and control.

$$n = \frac{16 \sigma^2}{\delta^2}$$

- 샘플 사이즈와 파라미터간 관계는 다음과 같음
    - $\sigma$가 크면 샘플이 더 필요하고, $\delta$가 크면 적은 샘플로도 충분함
    - $\sigma$는 존재하는 데이터로 계산할 수 있으나 $\delta$는 미리 계산할 수 없음
        - 이를 위해 마지막 파라미터인 MDE가 필요함
- **Minimum Detectable Effect**
    - 실무에서 효과를 거둘 수 있는 최소 수치를 의미함
    - **실험에서 수익률 0.1% 상승을 MDE로 설정할 수는 있지만, 실제로는 여러 이해관계자들과 논의를 통해서 정해야 함**
- 샘플 사이즈를 알게 되었다면 샘플 사이즈를 사용자 수로 나눠 테스트 수행 기간을 구할 수 있음
    - 일반적으로 2주 정도는 테스트하길 권장하지만 데이터를 더 수집할 수 있다면 길 수록 좋음

### 👨‍👩‍👧‍👦 Inference between control and treatment groups

- 일반적으론 실험군(Treatment Group)과 대조군(Control Group)을 임의 선택하여 분리함
    - **각 사용자들이 독립적이라는 가정을 하지만 때때로 이 독립 가정은 성립하지 않음**
    - 특히 페이스북, 링크드인, 트위터와 같은 소셜 네트워크나 우버, 리프트(Lyft), 에어비앤비와 같은 양면 시장(two-sided markets)에서 나타남
- **샘플 인터뷰 질문**
  
    >   *회사 X는 사용자당 포스트 등록 수를 높이기 위해 새로운 기능을 추가하여 이를 테스트하고 싶다.
    >   그들은 사용자를 임의로 실험군과 대조군으로 나누었다. 실험은 포스트 수 관점으로 1% 더 높게 나타났다.
    >   그렇다면 새로운 기능을 모든 사용자에게 런칭하였을 때 어떤 현상이 나타날 것으로 예상되는가?
    >   1%와 동일한 효과일지, 아니라면 그보다 많을지 혹은 적을지 생각해보시오. (Novelty effect는 없다고 가정함)*
    
    - 질문에 대한 대답은 1%보다 클 것으로 예상된다.
        - 일종의 출시 후 효과 (Post-launch Effect)로 볼 수 있음
    - 소셜 네트워크의 경우
        - 사용자의 행동은 그들의 소셜 서클(Social Circle) 내의 다른 사람들에게 많은 영향을 받음
            - **네트워크 효과 (Network Effect)**라고 부름
        - 이런 경우, 대조군 내의 사용자들이 실험군 내의 사용자에게 영향을 받게 됨
        - 실험군과 대조군의 차이가 실제 실험군 내에 발생하는 효과를 **과소평가**하게 됨 → 테스트보다 더 큰 효과가 발생함
    - 양면 마켓의 경우
        - 실험군과 대조군 사이의 간섭은 처치 효과(Treatment effect)를 평가할 때 편향을 야기함
            - 실험군과 대조군 사이의 리소스가 서로 공유되기 때문
            - 예를 들어 새로운 제품이 실험군 내의 운전수를 더 모으면, 반대로 대조군의 운전수이 줄어들게 됨
        - 소셜 네트워크의 경우와 반대로 실제 실험군 내에 발생하는 효과를 **과대평가**하게 됨 → 테스트보다 더 작은 효과가 발생함

### 🧩 How to deal with interference?

- 그렇다면 **실험군과 대조군 사이간 발생하는 파급효과(Spillover)를 방지할 수 있는 테스트**를 디자인하려면 어떻게 해야할까?
  
    >   *승객들에게 쿠폰을 제공하는 새로운 기능을 준비하고 있다.
    >   탑승 시 금액을 줄임으로써 탑승 수를 늘리는 것이 목적인데, 새로운 기능의 효과를 평가할 수 있는 테스트 전략을 개략적으로 설명하시오.*
    
    - 결국 주된 목적은 실험군과 대조군 내의 **사용자들을 고립(isolate)시키는 것**
    - 소셜 네트워크의 경우
        - **네트워크 클러스터 (Network Cluster)** 생성
            - 그룹 외부의 사용자보다 그룹 내부의 사용자들과 더 상호작용할 수 있는 클러스터를 생성하면 됨
            
            [Detecting interference: An A/B test of A/B tests](https://engineering.linkedin.com/blog/2019/06/detecting-interference--an-a-b-test-of-a-b-tests)
            
        - **Ego-cluster randomization**
    - 양면 마켓의 경우
        - **지리 기반 랜덤화 (Geo-based randomization)**
            - 지역 기반으로 실험군과 대조군을 나누는 방법으로, 각 그룹 내의 사용자를 고립시킬 수 있으나, 오히려 그룹간 편차가 높아질 수 있는 리스크가 있음 ← 각 마켓은 고객의 행동, 경쟁 업체 등으로 인해 매우 다른 성향을 나타내기 때문
        - **시간 기반 랜덤화 (Time-based randomization)**
            - 자주 쓰이진 않음
            - 이 방법은 처치 효과가 일시적으로 발생할 때 효과를 볼 수 있음

## 📊 Analyzing Results

### 🎭Novelty and Primacy Effects

- 테스트 후 발생할 수 있는 현상
    - **초두 효과 (Primacy Effect)**
        - 사용자들이 기존 제품/방식에 익숙하고 변화를 꺼려하여 발생하는 효과
    - **신기 효과 (Novelty Effect)**
        - 변화를 좋아하고 기존 제품/방식보다 새로운 기능을 선호하여 발생하는 효과
    - 그렇다고 해서 두 효과가 긴 시간 동안 지속되는 것은 아님 → 사용자의 행동 패턴이 안정화되기 때문
        - A/B 테스트가 처음에 크고 작은 효과가 발생하더라도 초두 효과나 신기 효과에 의한 것일 수 있음
- **샘플 인터뷰 질문**
  
    >   *새로운 기능을 위해 A/B 테스트를 수행하였고, 새로운 기능이 긍정적인 효과를 보여 모든 사용자에게 해당 기능을 제공하였다.
    >   하지만 일주일 후, 처치 효과가 급격하게 감소하는 것을 발견하였다.
    >   이유는 무엇인가?*
    
    - 신기 효과 때문임
- 잠재적인 이슈에는 어떻게 대처해야 하는가?
    - 이러한 효과의 가능성을 완전히 배제하는 것도 한 가지 방법임
        - 예를 들어 처음 제품을 사용하는 사용자들에 대해서만 테스트를 할 수도 있음
        (제품을 처음 사용하기 때문에 두 효과에 영향을 받지 않기 때문)
        - 만약 수행 중인 테스트가 있고, 결과에 초두 효과나 신기 효과가 있는지 분석하고 싶다면
            - 대조군 내의 신규 사용자의 결과와 실험군의 결과와 비교하여 신기 효과를 평가할 수 있음
            - 신규 사용자의 결과와 실험군 내 기존 사용자의 결과를 분석하여 두 효과의 영향을 실제로 평가할 수 있음
            

### 🧮 Multiple Testing Problem

- 종종 모든 기능들 중 최선을 찾기 위해 3개 이상의 변화를 주어 테스트하기도 함
    - 실험군이 하나보다 많은 상황 → 결과 검정 시 유의 수준을 단순하게 0.05로 맞출 수 없음
- **Multiple Testing Problem**
    - 실험군이 두 개 이상인 경우, False Discovery 확률이 높아짐
- **샘플 인터뷰 질문**
  
    >   *방문 페이지에 대해서 열 개의 서로 다른 버전을 가지고 테스트하고 있다.
    >   하나의 실험군이 p-value 0.05 미만으로 우수한 결과를 보였다.
    >   해당 실험군의 버전으로 페이지를 바꿀 것인가?*
    
    - Multiple Testing Problem으로 인해 바꾸어선 안됨
- Multiple Testing Problem 해결 방법
    - **Bonferroni Correction**
        - 유의 수준 0.05를 테스트 개수로 나눔
            - 샘플 인터뷰 질문의 경우에는 유의 수준이 0.005가 됨
        - 다만 보수적인 경향이 있다는 단점이 있음
    - **False Discovery Rate (FDR)**
        - $FDR = \mathbb{E}\text{[\# of false positive / \# of rejections]}$
        - 건수가 많을 때 유효한 방법

## ⚖️ Making Decisions

- A/B 테스트 후 통계 검정까지 완료하여 모든 사용자에게 새로운 기능을 제공하게 되더라도 간혹 **모순적인 결과(Contradicting Results)**가 발생함
- **샘플 인터뷰 질문**
  
    >   *A/B 테스트 후, 원하던대로 CTR이 증가하는 결과를 얻었으나 노출 횟수(impression)이 감소하는 것을 확인하였다.
    >   어떤 의사결정을 내릴 것인가?*
    
    - 실제 제품 출시에 대한 의사 결정은 매우 복잡할 수 밖에 없음
        - 구현 복잡도, 프로젝트 관리 공수, 고객 지원 비용, 유지보수 비용, 기회 비용 등을 모두 고려해야 함

---

#### **Reference**

-   [7 A/B Testing Questions and Answers in Data Science Interviews by Emma Ding](https://towardsdatascience.com/7-a-b-testing-questions-and-answers-in-data-science-interviews-eee6428a8b63#4848)

