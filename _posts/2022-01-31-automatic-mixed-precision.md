---
title: Automatic Mixed Precision (AMP)
tags: [amp, mixed-precision, pytorch, apex]
category: ML
aside:
  toc: true
show_category: true
---



<!--more-->

## ë“¤ì–´ê°€ë©°

ì „ PyTorchë³´ë‹¤ëŠ” Tensorflowë‚˜ Kerasë¥¼ ì´ìš©í•´ì„œ ëª¨ë¸ì„ êµ¬í˜„í•˜ê³¤ í–ˆëŠ”ë° ì´ë²ˆì— íˆ¬ì…ëœ í”„ë¡œì íŠ¸ì—ì„  ì „ë¶€ PyTorchë¡œ ì§œì—¬ìˆì—ˆìŠµë‹ˆë‹¤.
ì´ë¯¸ ì‘ì„±ëœ ëª¨ë¸ ì½”ë“œë¥¼ ë³´ëŠ”ë° NVIDIA APEXì˜ AMPë¥¼ ì‚¬ìš©í•˜ê³  ìˆë”êµ°ìš”.
ì˜ˆì „ë¶€í„° Low precisionì— ëŒ€í•´ì„œëŠ” ì§§ê²Œ ì•Œê³ ëŠ” ìˆì—ˆì§€ë§Œ ì´ë²ˆ ê¸°íšŒì— AMPì— ëŒ€í•´ì„œ ë” ìì„¸íˆ ê³µë¶€í•´ë†“ìœ¼ë©´ ë„ì›€ì´ ë˜ê² ë‹¤ëŠ” ìƒê°ì´ ë“¤ì–´ì„œ ë³¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

## ë¶€ë™ì†Œìˆ˜ì  (Floating Point)

ë³¸ê²©ì ì¸ ë‚´ìš©ì— ì•ì„œì„œ ë¶€ë™ì†Œìˆ˜ì ì— ëŒ€í•´ì„œ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤.
ë¶€ë™ì†Œìˆ˜ì ì—ì„œ 'ë¶€ë™'ì€ 'floating'ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ 'ì›€ì§ì´ì§€ ì•ŠëŠ”'ë‹¤ëŠ” ëœ»ì˜ ä¸å‹•ì´ ì•„ë‹Œ 'ë– ë‹¤ë‹Œë‹¤'ëŠ” ì˜ë¯¸ì˜ æµ®å‹•ì…ë‹ˆë‹¤.
ìœ„í‚¤í”¼ë””ì•„ì—ì„œëŠ” ë¶€ë™ì†Œìˆ˜ì ì„ ì•„ë˜ì™€ ê°™ì´ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤.

> ë¶€ë™ì†Œìˆ˜ì (æµ®å‹•å°æ•¸é», floating point)ì€ ì‹¤ìˆ˜ë¥¼ ì»´í“¨í„°ìƒì—ì„œ ê·¼ì‚¬í•˜ì—¬ í‘œí˜„í•  ë•Œ ì†Œìˆ˜ì ì˜ ìœ„ì¹˜ë¥¼ ê³ ì •í•˜ì§€ ì•Šê³  ê·¸ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ˜ë¥¼ ë”°ë¡œ ì ëŠ” ê²ƒ

<center>
  <figure>
    <img src="/assets/images/2022-01-31-automatic-mixed-precision/floating_point.png" alt="floating point" style="zoom:50%;" loading="lazy" />
    <figcaption style="text-align: center;">Image from <a href="https://towardsdatascience.com/understanding-mixed-precision-training-4b246679c7c4">[1]</a></figcaption>
  </figure>
</center>

ì¼ë°˜ì ìœ¼ë¡œ ë”¥ëŸ¬ë‹ì—ì„œëŠ” ì†Œìˆ˜ì ì„ ì²˜ë¦¬í•  ë•Œ 16ë¹„íŠ¸ë‚˜ 32ë¹„íŠ¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
ê°ê°ì„ FP16, FP32ë¡œ ë¶€ë¥´ëŠ”ë° ìœ„ ì´ë¯¸ì§€ì—ì„œë„ ì•Œ ìˆ˜ ìˆë“¯ì´ ë¶€í˜¸ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¹„íŠ¸ 1ê°œëŠ” ê³ ì •ì´ì§€ë§Œ, ì§€ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¹„íŠ¸, ê°€ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¹„íŠ¸ì˜ ìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤.

ê°„ë‹¨í•˜ê²Œ ì´ì•¼ê¸°í•´ì„œ ë” ë§ì€ ë¹„íŠ¸ê°€ ì‚¬ìš©ë  ìˆ˜ë¡ ì •í™•í•œ ì—°ì‚°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
í•˜ì§€ë§Œ ì—°ì‚° ì‹œì—ëŠ” ë” ì ì€ ë¹„íŠ¸ê°€ ì‚¬ìš©ë  ìˆ˜ë¡ ë¹ ë¥¸ ì†ë„ë¥¼ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ë”¥ëŸ¬ë‹ í•™ìŠµê³¼ ë¶€ë™ì†Œìˆ˜ì 

### Low Precisionì˜ ì¥ì 

ë³´í†µ ì•„ë¬´ëŸ° ì²˜ë¦¬ë¥¼ í•˜ì§€ ì•Šìœ¼ë©´ ëŒ€ë¶€ë¶„ì˜ ì†Œìˆ˜ëŠ” FP32ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
í•˜ì§€ë§Œ FP32ë¡œ ì²˜ë¦¬í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ê²Œ ë˜ë©´ ì–´ë–¤ ë¬¸ì œê°€ ë°œìƒí• ê¹Œìš”?

FP32ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê²Œ ë˜ë©´ Loss ê³„ì‚°, ê·¸ë¼ë””ì–¸íŠ¸ ê³„ì‚° ë“±ì´ ë§¤ìš° ë¬´ê±°ì›Œì§‘ë‹ˆë‹¤.
í•™ìŠµí•˜ëŠ”ë° ë„ˆë¬´ë‚˜ ì˜¤ëœ ì‹œê°„ì„ ì†Œë¹„í•˜ê²Œ ë©ë‹ˆë‹¤.
íŠ¹íˆ ëª¨ë¸ì˜ ë³µì¡ë„ê°€ ì ì  ì»¤ì ¸ê°€ëŠ” ìš”ì¦˜ ë§¤ìš° ì¹˜ëª…ì ì¸ ë¬¸ì œê°€ ë©ë‹ˆë‹¤.

ë¿ë§Œ ì•„ë‹ˆë¼ GPU ë©”ëª¨ë¦¬ ê´€ì ì—ì„œë„ BERT-Large ê°™ì€ í° ëª¨ë¸ë“¤ì€ ë‹¨ì¼ GPUë¡œ í•™ìŠµí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.
ê·¸ë ‡ë‹¤ê³  í•´ì„œ ê°€ëŠ¥í•œ ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ í•™ìŠµì„ í•˜ë”ë¼ë„ ì´ëŠ” ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬ì— ì œí•œì´ ìˆìŠµë‹ˆë‹¤.

FP16ìœ¼ë¡œ í•™ìŠµí•˜ê²Œ ë˜ë©´ ìœ„ ë¬¸ì œì ë“¤ì„ ì–´ëŠ ì •ë„ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ìš°ì„  ê³„ì‚°ì´ ê°„ë‹¨í•´ì§€ë©´ì„œ í•™ìŠµ ì†ë„ê°€ ë¹¨ë¼ì§€ê³  GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ê°ì†Œí•©ë‹ˆë‹¤.
FP16ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê²½ìš° FP32ë¡œ í•™ìŠµí•˜ëŠ” ê²½ìš° ëŒ€ë¹„ **ì ˆë°˜**ì˜ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.
ë©”ëª¨ë¦¬ì™€ ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ì„ ì ˆì•½í•  ìˆ˜ ìˆê³  ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ í¬ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ëŠ” ì¶”ê°€ì ì¸ í•™ìŠµ ì†ë„ ìƒìŠ¹ì„ ê¸°ëŒ€í•´ë³¼ ìˆ˜ ìˆì£ .

### Low Precisionì˜ ë¬¸ì œì 

ì´ë ‡ê²Œ ì¥ì ë§Œ ìˆëŠ” ê²ƒ ê°™ì§€ë§Œ ë‹¤ìŒ ì„¤ëª…í•  ë¬¸ì œê°€ êµ‰ì¥íˆ í° ë¬¸ì œê°€ ë©ë‹ˆë‹¤.
ë‹¹ì—°íˆ FP16ì€ FP32ë³´ë‹¤ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ìˆ«ìê°€ êµ‰ì¥íˆ ì ìŠµë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´ì„œ $1 + 10^{-4}$ë¥¼ FP16ê³¼ FP32ë¡œ ê³„ì‚°í•˜ë©´ ì–´ë–»ê²Œ ë ê¹Œìš”?
- ìš°ì„  FP32ë¡œ ê³„ì‚°í•˜ë©´ $1.0001$ë¡œ ì˜¬ë°”ë¥¸ ê°’ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **í•˜ì§€ë§Œ FP16ì—ì„  1**ë¡œ ê³„ì‚° ë©ë‹ˆë‹¤.

```python
>>> import numpy as np

>>> a = 1 + 10 ** -4

>>> np.float32(a)
1.0001
>>> np.float16(a)
1.0
```

ì´ì²˜ëŸ¼ FP16ì€ Dynamic rangeê°€ ë§¤ìš° ì‘ìŠµë‹ˆë‹¤.
ê·¸ëŸ¬ë‹¤ë³´ë‹ˆ Numeric overflowë‚˜ Numeric underflowê°€ ë°œìƒí•  ìˆ˜ ìˆì£ .
ê·¸ë ‡ê²Œ ë˜ë©´ ëª¨ë¸ í•™ìŠµ ì‹œ ì •í™•ë„ê°€ ë‚®ì•„ì§€ëŠ” ë¬¸ì œê°€ ìƒê¸°ê³ , ì‹¬ì§€ì–´ëŠ” loss functionì´ ìˆ˜ë ´í•˜ì§€ ì•Šê³  ë°œì‚°í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

<center>
  <figure>
    <img src="/assets/images/2022-01-31-automatic-mixed-precision/fp16_representation.png" alt="FP16 Representable range" style="zoom:75%;" loading="lazy" />
    <figcaption style="text-align: center;">Image from <a href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#training">[3]</a></figcaption>
  </figure>
</center>

ìœ„ ì´ë¯¸ì§€ëŠ” Multibox SSD networkë¥¼ FP32ë¡œ í•™ìŠµí–ˆì„ ë•Œ activationì˜ ê·¸ë¼ë””ì–¸íŠ¸ ê°’ì„ì„ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì…ë‹ˆë‹¤.
ë³´ì‹œë‹¤ì‹œí”¼ ê·¸ë¼ë””ì–¸íŠ¸ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë§¤ìš° ì‘ì€ ê°’ì…ë‹ˆë‹¤.
FP16ìœ¼ë¡œëŠ” ëŒ€ë¶€ë¶„ í‘œí˜„í•˜ê¸° ì–´ë µì£ .
ì‹¤ì œë¡œ ë¹¨ê°„ì„ ì„ ê¸°ì¤€ìœ¼ë¡œ ì™¼ìª½ì— ìˆëŠ” ê°’ë“¤ì€ FP16ì—ì„œ 0ì´ ë©ë‹ˆë‹¤.
ì˜¤ë¥¸ìª½ì— ìˆëŠ” ê°’ë“¤ ì¤‘ì—ì„œë„ FP16ì—ì„œëŠ” ëŒ€ë¶€ë¶„ì´ ë¹„ì •ê·œí™”ëœ ìˆ˜ (Denormalized numbers)ì´ê¸° ë•Œë¬¸ì— ì „ì²´ ê°’ë“¤ ì¤‘ì—ì„œ 5.3%ë§Œì´ í•™ìŠµì— ì‚¬ìš©ë˜ë©° ê²°êµ­ í•™ìŠµ ì‹œ ë°œì‚°í•˜ê²Œ ë©ë‹ˆë‹¤.
ê²Œë‹¤ê°€ ëŒ€ë¶€ë¶„ì˜ í‘œí˜„ ê°€ëŠ¥í•œ ë²”ìœ„ëŠ” ì“°ì´ì§€ë„ ì•ŠìŠµë‹ˆë‹¤.


### Mixed Precision

ìœ„ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ FP16ê³¼ FP32ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ **Mixed precision** ì…ë‹ˆë‹¤.
ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì— FP16ì„ ì‚¬ìš©í•˜ê³  íŠ¹ì • ê²½ìš°ì—ë§Œ FP32ë¥¼ ì‚¬ìš©í•´ì„œ í•™ìŠµí•˜ëŠ” ê²ƒì¸ë°ìš”.
ê³„ì‚° ì‹œ ìˆ˜ì¹˜ì ìœ¼ë¡œ ìœ„í—˜í•œ ê²½ìš°ë‚˜ ê°’ì„ ë°±ì—…í•˜ëŠ” ìš©ë„ë¡œ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
Numerical-safety ë ˆë²¨ì— ë§ì¶°ì„œ ì—°ì‚°ì„ ë¶„ë¥˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. [2]

- ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì „í•˜ê³  ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²½ìš° : Convolution, Matmul $\rightarrow$ FP16
- ìˆ˜ì¹˜ì  ì¤‘ë¦½ : Max, Min $\rightarrow$ ìƒí™©ì— ë”°ë¼ì„œ
- (ì¡°ê±´ë¶€ë¡œ) ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì „ : Activations $\rightarrow$ ìƒí™©ì— ë”°ë¼ì„œ
- ìˆ˜ì¹˜ì ìœ¼ë¡œ ìœ„í—˜í•¨ : Exp, Log, Pow, Softmax, Reduction Sum, Mean $\rightarrow$ FP32

Mixed precisionìœ¼ë¡œ í•™ìŠµí•˜ë©´ í•™ìŠµ ì†ë„ ë¿ë§Œ ì•„ë‹ˆë¼ ì„±ëŠ¥ í–¥ìƒê¹Œì§€ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [4] [5]

<center>
  <figure>
    <img src="/assets/images/2022-01-31-automatic-mixed-precision/mixed_precision_trains_faster.png" alt="TITLE" style="zoom:50%;" loading="lazy" />
    <figcaption style="text-align: center;">Image from <a href="https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/files/dusan_stosic_intro_to_mixed_precision_training.pdf">[4]</a></figcaption>
  </figure>
</center>

<center>
  <figure>
    <img src="/assets/images/2022-01-31-automatic-mixed-precision/mixed_precision_trains_accurately.png" alt="TITLE" style="zoom:50%;" loading="lazy" />
    <figcaption style="text-align: center;">Image from <a href="https://arxiv.org/abs/1710.03740">[5]</a></figcaption>
  </figure>
</center>

### Loss Scaling

<center>
  <figure>
    <img src="/assets/images/2022-01-31-automatic-mixed-precision/loss_scaling.png" alt="Loss scaling" style="zoom:75%;" loading="lazy" />
    <figcaption style="text-align: center;">Image from <a href="https://towardsdatascience.com/understanding-mixed-precision-training-4b246679c7c4">[1]</a></figcaption>
  </figure>
</center>

Mixed precisionìœ¼ë¡œ í•™ìŠµí•  ë•Œ Loss Scalingì€ ë‹¤ìŒì˜ ìˆœì„œë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. [3]

1. FP32 ê°€ì¤‘ì¹˜ ì¹´í”¼ ìƒì„±
2. ë§¤ ì´í„°ë ˆì´ì…˜ë§ˆë‹¤
    1. ê°€ì¤‘ì¹˜ì˜ FP16 ì¹´í”¼ ìƒì„±
    2. Forward propagation (FP16 ê°€ì¤‘ì¹˜, activations)
    3. Loss ê²°ê³¼ì— scaling factor $\mathcal{S}$ ê³±í•¨
        1. FP16ì´ê¸° ë•Œë¬¸ì— ìœ„ ê·¸ë¦¼ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ ë§¤ìš° ì‘ì€ ê°’ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ. Scaling factor ê³±í•´ì„œ ê°’ì„ ì„ì‹œë¡œ í¬ê²Œ ë§Œë“¦
    4. Backward propagation (FP16 ê°€ì¤‘ì¹˜, activations, ê·¸ë¼ë””ì–¸íŠ¸)
    5. ê°€ì¤‘ì¹˜ ê·¸ë¼ë””ì–¸íŠ¸ì— $\frac{1}{\mathcal{S}}$ ê³±í•¨
    6. ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸

### Dynamic Loss Scaling

ì¼ë°˜ì ì¸ Loss Scalingì€ ë³´ë‹¤ì‹œí”¼ Scaling factor $\mathcal{S}$ë¥¼ ì‚¬ì „ì— ì •í•´ì•¼ í•©ë‹ˆë‹¤.
í•˜ì§€ë§Œ scaling factorë¥¼ í° ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•œ í›„ ê°€ì¤‘ì¹˜ ê·¸ë¼ë””ì–¸íŠ¸ì— ì˜ëª»ëœ ê°’ì´ ë“¤ì–´ê°ˆ ë•Œ scaling factorë¥¼ ì¤„ì´ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ Dynamic Loss Scalingì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [3]

1. FP32 ê°€ì¤‘ì¹˜ ì¹´í”¼ ìƒì„±
2. Scaling factor $\mathcal{S}$ì„ í° ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
3. ë§¤ ì´í„°ë ˆì´ì…˜ë§ˆë‹¤
    1. ê°€ì¤‘ì¹˜ì˜ FP16 ì¹´í”¼ ìƒì„±
    2. Forward propagation (FP16 ê°€ì¤‘ì¹˜, activations)
    3. Loss ê²°ê³¼ì— Scaling factor $\mathcal{S}$ ê³±í•¨
    4. Backward propagation (FP16 ê°€ì¤‘ì¹˜, activations, ê·¸ë¼ë””ì–¸íŠ¸)
    5. ê°€ì¤‘ì¹˜ ê·¸ë¼ë””ì–¸íŠ¸ì— `Inf`ë‚˜ `NaN`ì´ ìˆëŠ” ê²½ìš°
        1. $\mathcal{S}$ë¥¼ ì¤„ì„
        2. ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ë¥¼ ìŠ¤í‚µí•˜ê³  ë‹¤ìŒ ì´í„°ë ˆì´ì…˜ìœ¼ë¡œ ë„˜ì–´ê°
    6. ê°€ì¤‘ì¹˜ ê·¸ë¼ë””ì–¸íŠ¸ì— $\frac{1}{\mathcal{S}}$ ê³±í•¨
    7. ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (ê·¸ë¼ë””ì–¸íŠ¸ í´ë¦¬í•‘ í¬í•¨í•  ìˆ˜ë„ ìˆê³ )
    8. ìµœê·¼ `N`ë²ˆì˜ ì´í„°ë ˆì´ì…˜ë™ì•ˆ `Inf`ë‚˜ `NaN`ì´ ì—†ì—ˆë‹¤ë©´ $\mathcal{S}$ ì¦ê°€

## NVIDIA APEX

### APEX

NVIDIAì—ì„œëŠ” Pytorchì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì—¬ëŸ¬ ê°€ì§€ í™•ì¥ ê¸°ëŠ¥ë“¤ì„ ì œê³µí•˜ëŠ” APEX (A PyTorch Extension) ê°œë°œí•˜ì—¬ ë°°í¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.
ì—¬ëŸ¬ ê°€ì§€ ê¸°ëŠ¥ì„ ì œê³µí•˜ì§€ë§Œ ê·¸ ì¤‘ì—ì„œë„ Mixed Precision Training ê¸°ëŠ¥ì„ ì œê³µí•˜ë©°, Automatic Mixed Precisionì„ ì œê³µí•©ë‹ˆë‹¤.

> ğŸ’¡ `apex.amp`Â is a tool to enable mixed precision training by changing only 3 lines of your script. Users can easily experiment with different pure and mixed precision training modes by supplying different flags toÂ `amp.initialize`.

NVIDIAì—ì„œëŠ” ë¦¬ëˆ…ìŠ¤ì— ì„¤ì¹˜í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•˜ê³  ìˆìœ¼ë©°, ëª¨ë“  ê¸°ëŠ¥ì„ ë‹¤ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” CUDAì™€ C++ ìµìŠ¤í…ì…˜ì„ ê°™ì´ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.

```bash
git clone https://github.com/NVIDIA/apex
cd apex
pip install -v --disable-pip-version-check --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./
```

Python-only ë¹Œë“œë„ ì§€ì›ì„ í•©ë‹ˆë‹¤.

```bash
pip install -v --disable-pip-version-check --no-cache-dir ./
```

### AMP ì‚¬ìš©í•˜ê¸°

AMPë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

{% highlight python linenos %}
from apex import amp
# Declare model and optimizer as usual, with default (FP32) precision
model = torch.nn.Linear(D_in, D_out).cuda()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)

# Allow Amp to perform casts as required by the opt_level
model, optimizer = amp.initialize(model, optimizer, opt_level="O1")
...
# loss.backward() becomes:
with amp.scale_loss(loss, optimizer) as scaled_loss:
    scaled_loss.backward()
...
{% endhighlight %}

Line 6ë¶€í„° AMPë¥¼ ì ìš©í•œ ì½”ë“œì…ë‹ˆë‹¤.
ìš°ì„  ìµœì í™” ëª¨ë“œ (optimization level)ì„ ì •í•´ì„œ AMPë¥¼ `initialize`í•˜ê³  loss scalingì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
Loss scalingì€ `amp.scale_loss().backward()`ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.

#### `opt_level`

ìµœì í™” ëª¨ë“œëŠ” `"O0"`, `"O1"`, `"O2"`, `"O3"`ë¡œ ì´ ë„¤ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.

- `O0` : FP32 í•™ìŠµ
  - AMPë¥¼ ì ìš©í•˜ì§€ ì•Šìœ¼ë©° ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì´ë¼ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.
- `O1` : Mixed Precision
  - ì¼ë°˜ì ìœ¼ë¡œ ê¶Œì¥í•˜ëŠ” ëª¨ë“œì…ë‹ˆë‹¤.
  - Tensor coreì— ì í•©í•œ ì—°ì‚°ë“¤ì€ FP16ìœ¼ë¡œ, ì •í™•í•œ ì—°ì‚°ì„ ìš”êµ¬í•˜ëŠ” ë¶€ë¶„ì€ FP32ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
  - Dynamic loss scalingì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
  - ëª¨ë¸ ê°€ì¤‘ì¹˜ëŠ” FP32ë¡œ ìœ ì§€ì‹œí‚µë‹ˆë‹¤.
- `O2` : "Almost FP16" Mixed Precision
  - `O1` ëª¨ë“œì™€ ë¹„êµí•˜ì—¬ FP16ì„ ë” ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤.
  - FP32 batchnorm, FP32 master weightë¥¼ ì œì™¸í•˜ê³ ëŠ” ëª¨ë‘ FP16ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
  - Dynamic loss scalingì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- `O3` : FP16 í•™ìŠµ
  - FP32 batchnormì„ ì œì™¸í•˜ê³  ëª¨ë‘ FP16ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë§¤ìš° ë¹ ë¥¸ ì†ë„ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.
  - ë§Œì•½ batch normalizationì„ ì‚¬ìš©í•˜ê³  ìˆë‹¤ë©´ ì¶”ê°€ ì¸ìë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. (`keep_batchnorm_fp32=True`)
  - Dynamic loss scalingì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

## References

[1] [Understanding Mixed Precision Training by Jonathan](https://towardsdatascience.com/understanding-mixed-precision-training-4b246679c7c4)

[2] [AMP Training Tutorial by Bojian Zheng](https://www.cs.toronto.edu/ecosystem/documents/AMP-Tutorial.pdf)

[3] [NVIDIA Deep Learning Performance Documentation](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)

[4] [Introduction to Mixed Precision Training by Dusan Stosic, NVIDIA](https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/files/dusan_stosic_intro_to_mixed_precision_training.pdf)

[5] [Micikevicius, Paulius, et al. "Mixed precision training." arXiv preprint arXiv:1710.03740 (2017).](https://arxiv.org/abs/1710.03740)